{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Video Classifier",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZT1LqwhGP6p",
        "colab_type": "text"
      },
      "source": [
        "#### If running in Google Colab, use the below route\n",
        "\n",
        "+ the below path should be changed to the exact path that you save this script\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_BJofpTGLyF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "os.chdir('/content/drive/My Drive/PLearning/Precision Learning-PSU Only/precisionLearning/code')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_z_SRtE1yAT",
        "colab_type": "text"
      },
      "source": [
        "#### Combine all the files and clean\n",
        "+ given files have different keys, first join the files with common 'video id' key\n",
        "+ second, join with object file with the common key of 'video_title'\n",
        "+ third, clean the file to have no more "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vB19x2qu2N2h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KMH7r_N1pau",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def data_combine(data_dir,init_path,marker):\n",
        "    import pandas as pd\n",
        "    import os\n",
        "    if init_path==None:\n",
        "\n",
        "        df=pd.DataFrame(columns=['Video id'])\n",
        "    else:\n",
        "        df=pd.read_csv(init_path)\n",
        "    for file in os.listdir(data_dir):\n",
        "        file=os.path.join(data_dir,file)\n",
        "        print(file)\n",
        "        data=pd.read_csv(file)\n",
        "        \n",
        "        df=pd.merge(data,df,how='outer',on='Video id')\n",
        "        print('It has shape {} after merging'.format(df.shape))\n",
        "    df.to_csv('../data/all_features_with_{}.csv'.format(marker),index=False)\n",
        "    return df\n",
        "\n",
        "df_185=data_combine('../data/185_row',None,'185row')\n",
        "\n",
        "def combine_object(df_185)\n",
        "    df_185=df_185.drop(['Unnamed: 0','Unnamed: 0_x','Unnamed: 0_y','Unnamed: 0_x','Unnamed: 0_y'],axis=1)\n",
        "    print(df_185.shape)#(390, 134)\n",
        "    df_185_dd=df_185.drop_duplicates()\n",
        "    print(df_185_dd.shape)#(266, 134)\n",
        "    df_185_dd.to_csv('all_features_with_185row_dd1.csv',index=False)\n",
        "    # df_185_dd=pd.read_csv('all_features_with_185row_dd1.csv')\n",
        "    print(df_185_dd.shape) \n",
        "    df_185_dd['video_title']=df_185_dd['video_title'].fillna('Missing')\n",
        "\n",
        "    df_185_dd['video_title']=df_185_dd['video_title'].str.replace(\"[#|.|,|\\||/|:]\",\"\").str.replace('(\\&#39;)|(\\&quot);',\"\").str.replace(\"&amp;\",\"_\")\n",
        "\n",
        "    df_185_dd['video_title']=df_185_dd['video_title'].str.replace('CCSS 7 SP 3 video 1 Mean Median Mode','CCSS 7 SP 3   video 1   Mean Median Mode')\n",
        "    df_185_dd['video_title']=df_185_dd['video_title'].str.replace('CCSS 7 SP 5 video 1 Understanding Probability','CCSS 7 SP 5   video 1   Understanding Probability')\n",
        "    df_185_dd['video_title']=df_185_dd['video_title'].str.replace('I don&39;t get Constant','I dont get Constant')\n",
        "    L1=list(df_185_dd['video_title'].unique())\n",
        "    L1.sort()\n",
        "    L1\n",
        "    label_df_narrow=pd.read_csv('../data/video_objects.csv')\n",
        "    df_185_all=pd.merge(df_185_dd,label_df_narrow,how='outer',on='video_title')\n",
        "    print(df_185_all.shape)#(269, 154)\n",
        "    df_185_all.to_csv('../data/all_features.csv',index=False)\n",
        "\n",
        "    return df_185_all\n",
        "\n",
        "def clean_data(df_185_all)\n",
        "    data0=pd.get_dummies(df_185_all[['IsFace']],columns=['IsFace'])\n",
        "    new_name=pd.Series(data0.columns.to_list()).str.replace('IsFace_','')\n",
        "    df_185_all0=df_185_all.drop('IsFace',axis=1)\n",
        "    print(df_185_all0.shape)\n",
        "    data0.columns=new_name\n",
        "    df_185_all=pd.concat([df_185_all0,data0],axis=1)\n",
        "    print(df_185_all.shape)\n",
        "    data0.head()\n",
        "    temp=df_185_all.drop(['code_desc','transcript'],axis=1)\n",
        "    df_185_all=df_185_all.drop_duplicates(subset='Video id')\n",
        "    print(df_185_all.shape)\n",
        "    df_185_all.head()#(190, 156)\n",
        "    df_185_all.to_csv('../data/all_features_dd.csv',index=False)\n",
        "    df_185_all1=df_185_all.dropna(subset=['Label'])\n",
        "    df_185_all1.to_csv('../data/all_features_noLabelNA.csv',index=False)\n",
        "    # aligning lables\n",
        "    conditions=[df_185_all1['Label']=='Yes',\n",
        "                df_185_all1['Label'].isin(['Not Relevant','Not Revelant']),\n",
        "                (df_185_all1['Label']=='Not Educational')]\n",
        "    choice=['Yes','No','No']\n",
        "    df_185_all1['label']=np.select(conditions, choice)\n",
        "    df_185_all1.drop('Label',axis=1,inplace=True)\n",
        "    df_185_all1.to_csv('../data/all_features_cleaned.csv',index=False)\n",
        "    return df_185_all1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_SbslIm4P2x",
        "colab_type": "text"
      },
      "source": [
        "#### Running 4 classifiers to see the best performance\n",
        "+ algorithms are used: KNNClassifer, Decision Tree, Random Forest, XGBoost\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvUwlLJN9YWH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(x_train,x_test,y_train,y_test):\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    import sklearn\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    from sklearn.neighbors import KNeighborsClassifier\n",
        "    from sklearn.ensemble import RandomForestClassifier\n",
        "    from sklearn.tree import DecisionTreeClassifier\n",
        "    from xgboost import XGBClassifier\n",
        "    from sklearn.metrics import confusion_matrix\n",
        "    from sklearn.metrics import classification_report\n",
        "    import scikitplot as skplt\n",
        "    import matplotlib.pyplot as plt\n",
        "    %matplotlib inline\n",
        "    import warnings\n",
        "    warnings.filterwarnings('ignore')\n",
        "# name of models\n",
        "    model_name_list=['KNN','Decision Tree', 'Random Forest','XGboost']\n",
        "\n",
        "    #instantiate the models\n",
        "    model1=KNeighborsClassifier()\n",
        "    model2=DecisionTreeClassifier(min_samples_split=2,random_state=0)\n",
        "    model3=RandomForestClassifier(min_samples_split=2,random_state=0)\n",
        "    model4=XGBClassifier(\n",
        "                 learning_rate =0.01,\n",
        "                #  max_depth=4,\n",
        "                 min_child_weight=1,\n",
        "                 gamma=0,\n",
        "                 subsample=0.8,\n",
        "                 colsample_bytree=0.8,\n",
        "                 objective='binary:logistic',\n",
        "                 n_jobs=4,\n",
        "                 scale_pos_weight=1,\n",
        "                 reg_alpha=0,\n",
        "                 reg_lambda=1,\n",
        "                 seed=100)\n",
        "\n",
        "    #dataframe for results\n",
        "#     results=pd.DataFrame(columns=['y_proba'],index=model_name_list)\n",
        "    \n",
        "    #train and predict with each model\n",
        "    for i, model in enumerate([model1,model2, model3,model4]):\n",
        "        model.fit(x_train, y_train)\n",
        "        y_proba=model.predict_proba(x_test)\n",
        "        #metrics\n",
        "\n",
        "        y_pred=model.predict(x_test)\n",
        "        #insert results into the dataframe\n",
        "        model_name=model_name_list[i]\n",
        "        print('----'+ model_name+'-----')\n",
        "        cm=confusion_matrix(y_test,y_pred)\n",
        "        cm=pd.DataFrame(cm,columns=['Predicted No','Predicted Yes'],index=['Actual No','Actual Yes'])\n",
        "        print(cm)\n",
        "#         results.ix[model_name,:]=[y_proba]\n",
        "        skplt.metrics.plot_roc(y_test,y_proba,figsize=(8,6),title='{} ROC Curves'.format(model_name))\n",
        "        plt.show\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdyhafqV9YZQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%pip install scikit-plot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1avy9gk-a5a",
        "colab_type": "text"
      },
      "source": [
        "##### No feature engineering\n",
        "+ without any feature engineering, the model performance is 0.92"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxlWdelk9YPR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "na_df=pd.DataFrame(df_185_all1.isna().sum(),columns=['NA_num']).reset_index()\n",
        "na_df=na_df.sort_values('NA_num',ascending=False)\n",
        "valid_feat=na_df[na_df['NA_num']<20]['index'].unique()\n",
        "# below is to make the features visible, you can also just use valid_feat from above as a variable list\n",
        "valid_feat=['median_flatness', 'count of number of 1s/total windows', 'Favorites', 'pitch_tuning', 'Comments', 'Dislikes', \n",
        "            'Likes', 'Views', 'Paper', 'Alphabet', 'Measurements', 'Flyer', 'Electronics', 'Driving License', \n",
        "            'Face', 'Plan', 'Handwriting', 'Diagram', 'Document', 'White Board', 'Human', 'Person', 'Word', 'Page', 'Symbol',\n",
        "            'Number', 'Plot', 'Text', 'wpm', 'Profile faces', 'OtherP',   'Front faces', 'Apostro',\n",
        "            'Parenth', 'Quote',  'adverb', 'you', 'shehe', 'they', 'ipron', 'article', 'prep', 'auxverb', 'conj',\n",
        "            'Dash', 'negate', 'verb', 'adj', 'compare', 'interrog', 'number', 'quant', 'we', 'i', 'ppron', 'pronoun', \n",
        "             'label', 'WC', 'Analytic', 'Clout', 'Authentic', 'Tone', 'WPS', 'Sixltr', 'Dic', 'function', 'affect', 'posemo', 'negemo', 'power', 'focuspast', \n",
        "            'focuspresent', 'focusfuture', 'relativ', 'motion', 'space', 'time', 'work', 'leisure', 'money', 'informal', \n",
        "            'netspeak', 'assent', 'nonflu', 'AllPunc', 'reward', 'achieve', 'social', 'affiliation', 'male', 'cogproc', \n",
        "            'insight', 'cause', 'discrep', 'tentat', 'certain', 'differ', 'percept', 'see', 'hear', 'feel', 'bio', \n",
        "            'health', 'drives', 'Number of speakers','Front face','Side profile', 'Voice only']\n",
        "print(len(valid_feat))\n",
        "valid_data=df_185_all1[valid_feat].fillna(0)\n",
        "valid_data.to_csv('../data/training_data_{}feat.csv'.format(len(valid_feat)),index=False)\n",
        "X=valid_data.drop('label',axis=1)\n",
        "Y=valid_data['label']\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train,x_test,y_train,y_test=train_test_split(X,Y,test_size=0.2,random_state=88)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MvDYhZQ92Ss",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "evaluate(x_train,x_test,y_train,y_test) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8ImYaLR-lCn",
        "colab_type": "text"
      },
      "source": [
        "##### Toss sparse features and kept 73 features\n",
        "\n",
        "+ kept the 73 features that have less than 100 zeros and got 0.90 ROC Curve"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZ0s_q70-lFC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_richData(valid_data):\n",
        "    zeros=pd.DataFrame((valid_data==0).astype(int).sum(axis=0),columns=['num_zeros']).reset_index()\n",
        "    zeros=zeros.sort_values('num_zeros',ascending=False)\n",
        "    zeros.to_csv('../data/num_zeros.csv',index=False)\n",
        "    below_100=zeros[zeros['num_zeros']<100]\n",
        "    print('{} features that have less than 100 zeros'.format(len(below_100)))\n",
        "    return lsit(below_100['index'].unique())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAKxZMas-lIA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "below_100=get_richData(valid_data)\n",
        "# below are the output of the below_100 list, given more features will be added, the variable 'below_100' should supercede the below hard-coded ones\n",
        "# ['Number of speakers','assent', 'bio', 'leisure', 'Word', 'negemo', 'Plot', 'hear',\n",
        "#        'negate', 'they', 'nonflu', 'IsFace_digit', 'i', 'work',\n",
        "#        'informal', 'count of number of 1s/total windows', 'achieve',\n",
        "#        'reward', 'see', 'you', 'percept', 'motion', 'focuspast',\n",
        "#        'certain', 'posemo', 'focusfuture', 'power', 'we', 'affect',\n",
        "#        'discrep', 'insight', 'tentat', 'differ', 'cause',\n",
        "#        'median_flatness', 'affiliation', 'pitch_tuning', 'Text', 'adj',\n",
        "#        'compare', 'Views', 'interrog', 'time', 'quant', 'social', 'ppron',\n",
        "#        'number', 'drives', 'wpm', 'Apostro', 'ipron', 'adverb', 'cogproc',\n",
        "#        'article', 'Authentic', 'prep', 'auxverb', 'WPS', 'Sixltr', 'Dic',\n",
        "#        'function', 'Clout', 'Analytic', 'WC', 'verb', 'focuspresent',\n",
        "#        'pronoun', 'relativ', 'space', 'Tone', 'conj', 'AllPunc', 'label']\n",
        "FE_nonZero=valid_data[below_100]\n",
        "X=FE_nonZero.drop('label',axis=1)\n",
        "Y=FE_nonZero['label']\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train,x_test,y_train,y_test=train_test_split(X,Y,test_size=0.2,random_state=88)\n",
        "evaluate(x_train,x_test,y_train,y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbHlbMOV_aiT",
        "colab_type": "text"
      },
      "source": [
        "##### Model with derived features and valid features\n",
        "+ model performance is 0.98\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-Key3AE_ak1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def text_feat(data,feature):\n",
        "  data[feature]=data[feature].astype(str)\n",
        "  data['word_count'+\"_\"+feature]=data[feature].apply(lambda x: len(x.split(\" \")))\n",
        "  data['char_count'+\"_\"+feature]=data[feature].str.len()\n",
        "  def avg_word(word):\n",
        "   words=word.split( )\n",
        "   return sum(len(word) for word in words)/len(words)\n",
        "  data['avg_word'+\"_\"+feature]=data[feature].apply(lambda x: avg_word(x))\n",
        "  data['uppercase'+\"_\"+feature]=data[feature].apply(lambda x: len([x for x in x.split() if x.isupper()]))\n",
        "  data['lowercase'+\"_\"+feature]=data[feature].apply(lambda x: len([x for x in x.split() if x.islower()]))\n",
        "  return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-l_JZN5_anK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def get_derived(clean_data_path)\n",
        "    df_185_all1=pd.read_csv(clean_data_path)\n",
        "    df_185_all1.columns\n",
        "    text_feat(df_185_all1,'code_desc')\n",
        "    text_feat(df_185_all1,'video_desc')\n",
        "    df_185_all1=text_feat(df_185_all1,'video_title')\n",
        "    df_185_all1['codeVideoDesc_WC_dif'] =df_185_all1['word_count_code_desc']- df_185_all1['word_count_video_desc']\n",
        "    df_185_all1['codeVideoDesc_CC_dif']=df_185_all1['char_count_code_desc']- df_185_all1['char_count_video_desc']\n",
        "    df_185_all1['codeVideoDesc_AvgW_dif']=df_185_all1['avg_word_code_desc']- df_185_all1['avg_word_video_desc']\n",
        "    df_185_all1['codeVideoDesc_UC_dif']=df_185_all1['uppercase_code_desc']- df_185_all1['uppercase_video_desc']\n",
        "    df_185_all1['codeVideoDesc_LC_dif']=df_185_all1['lowercase_code_desc']- df_185_all1['lowercase_video_desc']\n",
        "    df_185_all1['uppercase_video_title']=df_185_all1['video_title'].apply(lambda x: len([x for x in x.split() if x.isupper()]))\n",
        "    df_185_all1.head(5)\n",
        "    df_185_all1['lowercase_video_title']=df_185_all1['video_title'].apply(lambda x: len([x for x in x.split() if x.islower()]))\n",
        "    df_185_all1['codeTitle_WC_dif'] =df_185_all1['word_count_code_desc']- df_185_all1['word_count_video_title']\n",
        "    df_185_all1['codeTitle_CC_dif']=df_185_all1['char_count_code_desc']- df_185_all1['char_count_video_title']\n",
        "    df_185_all1['codeTitle_AvgW_dif']=df_185_all1['avg_word_code_desc']- df_185_all1['avg_word_video_title']\n",
        "    df_185_all1['codeTitle_UC_dif']=df_185_all1['uppercase_code_desc']- df_185_all1['uppercase_video_title']\n",
        "    df_185_all1['codeTitle_LC_dif']=df_185_all1['lowercase_code_desc']- df_185_all1['lowercase_video_title']\n",
        "    df_185_all1.to_csv('../data/all_features_wDerived_cleaned.csv',index=False)\n",
        "    return df_185_all1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVZGa-Qh_4aq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clean_data_path='../data/all_features_cleaned.csv'\n",
        "df_185_all1=get_derived(clean_data_path)\n",
        "# below is an eyeballed feature list\n",
        "text_feat=['word_count_code_desc','char_count_code_desc','avg_word_code_desc',\n",
        " 'uppercase_code_desc','lowercase_code_desc','word_count_video_desc',\n",
        " 'char_count_video_desc','avg_word_video_desc','uppercase_video_desc',\n",
        " 'lowercase_video_desc','word_count_video_title','char_count_video_title',\n",
        " 'avg_word_video_title', 'uppercase_video_title','lowercase_video_title',\n",
        " 'codeVideoDesc_WC_dif','codeVideoDesc_CC_dif','codeVideoDesc_AvgW_dif',\n",
        " 'codeVideoDesc_UC_dif','codeVideoDesc_LC_dif','codeTitle_WC_dif',\n",
        " 'codeTitle_CC_dif','codeTitle_AvgW_dif','codeTitle_UC_dif',\n",
        " 'codeTitle_LC_dif']\n",
        "valid_custom_data=df_185_all1[text_feat+valid_feat].fillna(0)\n",
        "print(valid_custom_data.shape) #(259, 131)\n",
        "valid_custom_data.to_csv('../data/valid_derived_{}feat.csv'.format(valid_custom_data.shape[1]),index=False)\n",
        "X=valid_custom_data.drop('label',axis=1)\n",
        "Y=valid_custom_data['label']\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train,x_test,y_train,y_test=train_test_split(X,Y,test_size=0.2,random_state=88)\n",
        "evaluate(x_train,x_test,y_train,y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuSgQeY8By1H",
        "colab_type": "text"
      },
      "source": [
        "##### Model with text features and features with below 100 zeros\n",
        "+ model performance is 0.97 (last run on 5/13/2020 after added number of speakers)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pX8IdS43B0IJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "valid_custom_NonZero=df_185_all1[text_feat+below_100].fillna(0)\n",
        "print(valid_custom_NonZero.shape)#(259, 98)\n",
        "valid_custom_NonZero.to_csv('../data/below100Zero_custom_{}feat.csv'.format(valid_custom_NonZero.shape[1]),index=False)\n",
        "X=valid_custom_NonZero.drop('label',axis=1)\n",
        "Y=valid_custom_NonZero['label']\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train,x_test,y_train,y_test=train_test_split(X,Y,test_size=0.2,random_state=88)\n",
        "evaluate(x_train,x_test,y_train,y_test) #0.71"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilQGxA_ABy8H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Wbo8pNHBy_u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}